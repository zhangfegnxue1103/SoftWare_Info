DELETE hamlet_*
DELETE _template/*

参考地址：
https://www.elastic.co/guide/en/elasticsearch/reference/7.2/analysis.html
https://www.elastic.co/guide/en/elasticsearch/reference/7.2/analysis-custom-analyzer.html

# 01 Create the index `hamlet_1` with one primary shard and no replicas
#创建索引hamlet-1，1主分片且无副本，
# Define a mapping for the default type "_doc" of `hamlet_1`, so 
#定义一个mapping，默认类型doc，
#    that (i) the type has three fields, named `speaker`, 
#    `line_number`, and `text_entry`, (ii) `text_entry` is 
#    associated with the language "english" analyzer
#有3个字段，speaker、line_number、text_entry，
#text_entry的分词器使用english。
# Add some documents to `hamlet_1` by running the following command。
#添加一些文档到hamlet_1中，运行bulk命令。

#创建索引mappings 和 settings
PUT hamlet_1
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  },
  "mappings": {
    "properties": {
      "speaker":{"type": "text"},
      "line_number":{"type": "text"},
      "text_entry":{"type": "text","analyzer": "english"}
    }
  }
}

#放入数据到hamlet_1
PUT hamlet_1/_bulk
{"index":{"_index":"hamlet_1","_id":0}}
{"line_number":"1.1.1","speaker":"BERNARDO","text_entry":"Whos there?"}
{"index":{"_index":"hamlet_1","_id":1}}
{"line_number":"1.1.2","speaker":"FRANCISCO","text_entry":"Nay, answer me: stand, and unfold yourself."}
{"index":{"_index":"hamlet_1","_id":2}}
{"line_number":"1.1.3","speaker":"BERNARDO","text_entry":"Long live the king!"}
{"index":{"_index":"hamlet_1","_id":3}}
{"line_number":"1.2.1","speaker":"KING CLAUDIUS","text_entry":"Though yet of Hamlet our dear brothers death"}


# 02 Create the index `hamlet_2` with one primary shard and no replicas
# 创建索引hmalet_2，1主0分片
# Add to `hamlet_2` a custom analyzer named `shy_hamlet_analyzer`, 
# 使用自定义分词器shy_hamlet_analyzer
#    consisting of 
#    (i)   a char filter to replace the characters "Hamlet" with "
#         [CENSORED]",  
#      配置char filter部分 字符hamlet替换为CENSORED
#    (ii)  a tokenizer to split tokens on whitespaces and columns,  
#      配置tokenizer部分 按空格和.来分割
#    (iii) a token filter to ignore any token with less than 5  characters 
#      配置filter部分，字符数小于5则忽略token
# Define a mapping for the default type "_doc" of `hamlet_2`, so 
#    that 
# 定义一个mapping，默认类型doc，索引hamlet_2，
#	(i) the type has one field named `text_entry`, 
#        包含text_entry名字的字段
#   (ii)`text_entry` is associated with the `shy_hamlet_analyzer` 
#        字段 text_entry 分配给分词器shy_hamlet_analyzer
#    created in the previous step

PUT hamlet_2
{
  "settings": {
    "number_of_replicas": 0,
    "number_of_shards": 1,
    "analysis": {
      "analyzer": {
        "shy_hamlet_analyzer": {
          "type": "custom",
          "tokenizer": "whitespace",
          "char_filter": [
            "my_char_filter"
          ],
          "filter": [
            "my_condition"
          ]
        },
        "char_filter": {
          "my_char_filter": {
            "type": "mapping",
            "mappings": [
              "Hamlet => [CENSORED]"
            ]
          }
        },
        "filter": {
          "my_condition": {
            "type": "condition",  
            "filter": ["lowercase"],
            "script": {
              "source": "token.getTerm().length() < 5"
            }
          }
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "text_entry":{
        "type": "text",
        "analyzer": "shy_hamlet_analyzer"
      }
    }
  }
}

---------------------------------------------------------------------------------------------------
报错信息：
{
  "error": {
    "root_cause": [
      {
        "type": "remote_transport_exception",
        "reason": "[node3][192.168.56.232:9300][indices:admin/create]"
      }
    ],
    "type": "illegal_argument_exception",
    "reason": "analyzer [filter] must specify either an analyzer type, or a tokenizer"
  },
  "status": 400
}
---------------------------------------------------------------------------------------------------

# 03 Reindex the `text_entry` field of `hamlet_1` into `hamlet_2`
#    重建索引hamlet_1的字段 text_entry到索引hamlet_2，
# Verify that documents have been reindexed to `hamlet_2` as
#    验证这个文档已经重建索引到hamlet_2中
# expected - e.g., by searching for "censored" into the 
# `text_entry` field
#  搜索指定字段和和值，达到预期值   

POST _reindex
{
"source": {
  "index": "hamlet_1"
},
"dest": {
  "index": "hamlet_2"
}
}
POST hamlet_2/_search

POST hamlet_2/_analyze
{
  "analyzer": "shy_hamlet_analyzer",
  "text": "Though yet of Hamlet our dear brothers death"
}

POST hamlet_2/_search
{
  "query": {
    "match": {
      "text_entry": "[CENSORED]"
    }
  }
}